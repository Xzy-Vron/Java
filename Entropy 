%% Initialization
clc;
clear all;
close all;

%% Part 1: Calculate Entropies for a Specific Channel

% --- Pre-defined Inputs ---
% The 'input()' function was removed to fix the error.
% Change these values directly to test different scenarios.
px_1 = 0.6;         % Probability of x1
py1_given_x1 = 0.9; % P(y1|x1) - Probability of receiving y1 given x1 was sent
py1_given_x2 = 0.2; % P(y1|x2) - Probability of receiving y1 given x2 was sent

% --- Setup probability vectors from pre-defined inputs ---
px(1) = px_1;
px(2) = 1 - px(1);

py_given_x(1,1) = py1_given_x1;
py_given_x(1,2) = 1 - py_given_x(1,1);
py_given_x(2,1) = py1_given_x2;
py_given_x(2,2) = 1 - py_given_x(2,1);

% --- Calculations ---
% Calculate joint probability P(x,y) = P(y|x) * P(x)
pxy = zeros(2,2);
for i = 1:2
for j = 1:2
pxy(i,j) = py_given_x(i,j) * px(i);
end
end

% Calculate output probability P(y) by summing columns of P(x,y)
py = sum(pxy, 1);

Hx = -sum(px .* log2(px));
Hy = -sum(py .* log2(py));

H_y_given_x = 0;
for i = 1:2
for j = 1:2
if pxy(i,j) > 0 % Avoid log2(0)
H_y_given_x = H_y_given_x - pxy(i,j) * log2(py_given_x(i,j));
end
end
end

H_xy = 0;
for i = 1:2
for j = 1:2
if pxy(i,j) > 0 % Avoid log2(0)
H_xy = H_xy - pxy(i,j) * log2(pxy(i,j));
end
end
end
% Note: H(x,y) can also be calculated as H(x) + H(y|x)

I_xy = Hy - H_y_given_x;
% Note: Mutual information can also be calculated as H(x) - H(x|y) or H(x) + H(y) - H(x,y)

% --- Display Results ---
fprintf('\n--- Calculated Results ---\n');
disp('Source Probability P(x):'); disp(px);
disp('Channel Matrix P(y|x):'); disp(py_given_x);
disp('Joint Probability P(x,y):'); disp(pxy);
disp('Output Probability P(y):'); disp(py);
fprintf('Source Entropy H(x) = %f bits/symbol\n', Hx);
fprintf('Output Entropy H(y) = %f bits/symbol\n', Hy);
fprintf('Conditional Entropy H(y|x) = %f bits/symbol\n', H_y_given_x);
fprintf('Joint Entropy H(x,y) = %f bits/symbol\n', H_xy);
fprintf('Mutual Information I(x,y) = %f bits/symbol\n', I_xy);

%% Part 2: Plot Entropy of a Binary Source
p = 0.001:0.001:0.999; % Avoid 0 and 1 to prevent log2(0) = -Inf
q = 1 - p;

H = -p.*log2(p) - q.*log2(q);

figure;
plot(p, H, 'LineWidth', 2);
grid on;
xlabel('Probability p(x=1)');
ylabel('Entropy H(x) in bits/symbol');
title('Entropy of a Binary Source');
axis([0 1 0 1.1]); % Set axis limits for clarity
